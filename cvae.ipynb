{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7fbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a94f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['strength'])\n",
    "y = df_train['strength']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a535bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, cond_dim=1, latent_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(x_dim + cond_dim, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, x_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        xc = torch.cat([x, c], dim=1)\n",
    "        h = self.encoder(xc)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        zc = torch.cat([z, c], dim=1)\n",
    "        return self.decoder(zc)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z, c)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a51354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_recon, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='mean')\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f5bac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 181.0334\n",
      "Epoch 1, Loss: 69.7623\n",
      "Epoch 2, Loss: 21.7240\n",
      "Epoch 3, Loss: 5.3488\n",
      "Epoch 4, Loss: 1.6912\n",
      "Epoch 5, Loss: 1.5183\n",
      "Epoch 6, Loss: 1.5001\n",
      "Epoch 7, Loss: 1.3973\n",
      "Epoch 8, Loss: 1.3496\n",
      "Epoch 9, Loss: 1.3214\n",
      "Epoch 10, Loss: 1.3007\n",
      "Epoch 11, Loss: 1.2727\n",
      "Epoch 12, Loss: 1.3130\n",
      "Epoch 13, Loss: 1.2803\n",
      "Epoch 14, Loss: 1.2677\n",
      "Epoch 15, Loss: 1.2388\n",
      "Epoch 16, Loss: 1.1498\n",
      "Epoch 17, Loss: 1.1398\n",
      "Epoch 18, Loss: 1.1239\n",
      "Epoch 19, Loss: 1.0965\n",
      "Epoch 20, Loss: 1.0506\n",
      "Epoch 21, Loss: 1.0512\n",
      "Epoch 22, Loss: 0.9868\n",
      "Epoch 23, Loss: 0.9698\n",
      "Epoch 24, Loss: 0.9126\n",
      "Epoch 25, Loss: 0.8953\n",
      "Epoch 26, Loss: 0.8913\n",
      "Epoch 27, Loss: 0.8652\n",
      "Epoch 28, Loss: 0.8384\n",
      "Epoch 29, Loss: 0.8137\n",
      "Epoch 30, Loss: 0.8172\n",
      "Epoch 31, Loss: 0.8099\n",
      "Epoch 32, Loss: 0.8138\n",
      "Epoch 33, Loss: 0.7615\n",
      "Epoch 34, Loss: 0.7951\n",
      "Epoch 35, Loss: 0.7744\n",
      "Epoch 36, Loss: 0.7515\n",
      "Epoch 37, Loss: 0.7543\n",
      "Epoch 38, Loss: 0.7330\n",
      "Epoch 39, Loss: 0.7430\n",
      "Epoch 40, Loss: 0.7321\n",
      "Epoch 41, Loss: 0.7230\n",
      "Epoch 42, Loss: 0.7286\n",
      "Epoch 43, Loss: 0.7294\n",
      "Epoch 44, Loss: 0.7128\n",
      "Epoch 45, Loss: 0.7223\n",
      "Epoch 46, Loss: 0.7132\n",
      "Epoch 47, Loss: 0.7042\n",
      "Epoch 48, Loss: 0.6959\n",
      "Epoch 49, Loss: 0.7065\n",
      "Epoch 50, Loss: 0.6876\n",
      "Epoch 51, Loss: 0.6846\n",
      "Epoch 52, Loss: 0.6962\n",
      "Epoch 53, Loss: 0.6785\n",
      "Epoch 54, Loss: 0.6830\n",
      "Epoch 55, Loss: 0.6685\n",
      "Epoch 56, Loss: 0.6823\n",
      "Epoch 57, Loss: 0.6810\n",
      "Epoch 58, Loss: 0.6876\n",
      "Epoch 59, Loss: 0.6728\n",
      "Epoch 60, Loss: 0.6667\n",
      "Epoch 61, Loss: 0.6643\n",
      "Epoch 62, Loss: 0.6703\n",
      "Epoch 63, Loss: 0.6643\n",
      "Epoch 64, Loss: 0.6750\n",
      "Epoch 65, Loss: 0.6566\n",
      "Epoch 66, Loss: 0.6767\n",
      "Epoch 67, Loss: 0.6644\n",
      "Epoch 68, Loss: 0.6675\n",
      "Epoch 69, Loss: 0.6713\n",
      "Epoch 70, Loss: 0.6562\n",
      "Epoch 71, Loss: 0.6741\n",
      "Epoch 72, Loss: 0.6642\n",
      "Epoch 73, Loss: 0.6693\n",
      "Epoch 74, Loss: 0.6669\n",
      "Epoch 75, Loss: 0.6637\n",
      "Epoch 76, Loss: 0.6581\n",
      "Epoch 77, Loss: 0.6614\n",
      "Epoch 78, Loss: 0.6522\n",
      "Epoch 79, Loss: 0.6558\n",
      "Epoch 80, Loss: 0.6647\n",
      "Epoch 81, Loss: 0.6588\n",
      "Epoch 82, Loss: 0.6636\n",
      "Epoch 83, Loss: 0.6576\n",
      "Epoch 84, Loss: 0.6657\n",
      "Epoch 85, Loss: 0.6659\n",
      "Epoch 86, Loss: 0.6516\n",
      "Epoch 87, Loss: 0.6548\n",
      "Epoch 88, Loss: 0.6602\n",
      "Epoch 89, Loss: 0.6518\n",
      "Epoch 90, Loss: 0.6568\n",
      "Epoch 91, Loss: 0.6511\n",
      "Epoch 92, Loss: 0.6568\n",
      "Epoch 93, Loss: 0.6451\n",
      "Epoch 94, Loss: 0.6453\n",
      "Epoch 95, Loss: 0.6503\n",
      "Epoch 96, Loss: 0.6513\n",
      "Epoch 97, Loss: 0.6504\n",
      "Epoch 98, Loss: 0.6473\n",
      "Epoch 99, Loss: 0.6470\n",
      "Epoch 100, Loss: 0.6467\n",
      "Epoch 101, Loss: 0.6472\n",
      "Epoch 102, Loss: 0.6402\n",
      "Epoch 103, Loss: 0.6399\n",
      "Epoch 104, Loss: 0.6553\n",
      "Epoch 105, Loss: 0.6502\n",
      "Epoch 106, Loss: 0.6490\n",
      "Epoch 107, Loss: 0.6447\n",
      "Epoch 108, Loss: 0.6409\n",
      "Epoch 109, Loss: 0.6432\n",
      "Epoch 110, Loss: 0.6536\n",
      "Epoch 111, Loss: 0.6395\n",
      "Epoch 112, Loss: 0.6459\n",
      "Epoch 113, Loss: 0.6432\n",
      "Epoch 114, Loss: 0.6478\n",
      "Epoch 115, Loss: 0.6374\n",
      "Epoch 116, Loss: 0.6388\n",
      "Epoch 117, Loss: 0.6427\n",
      "Epoch 118, Loss: 0.6423\n",
      "Epoch 119, Loss: 0.6484\n",
      "Epoch 120, Loss: 0.6460\n",
      "Epoch 121, Loss: 0.6433\n",
      "Epoch 122, Loss: 0.6465\n",
      "Epoch 123, Loss: 0.6312\n",
      "Epoch 124, Loss: 0.6364\n",
      "Epoch 125, Loss: 0.6437\n",
      "Epoch 126, Loss: 0.6298\n",
      "Epoch 127, Loss: 0.6395\n",
      "Epoch 128, Loss: 0.6359\n",
      "Epoch 129, Loss: 0.6340\n",
      "Epoch 130, Loss: 0.6399\n",
      "Epoch 131, Loss: 0.6477\n",
      "Epoch 132, Loss: 0.6432\n",
      "Epoch 133, Loss: 0.6408\n",
      "Epoch 134, Loss: 0.6494\n",
      "Epoch 135, Loss: 0.6412\n",
      "Epoch 136, Loss: 0.6388\n",
      "Epoch 137, Loss: 0.6341\n",
      "Epoch 138, Loss: 0.6318\n",
      "Epoch 139, Loss: 0.6326\n",
      "Epoch 140, Loss: 0.6372\n",
      "Epoch 141, Loss: 0.6414\n",
      "Epoch 142, Loss: 0.6367\n",
      "Epoch 143, Loss: 0.6407\n",
      "Epoch 144, Loss: 0.6493\n",
      "Epoch 145, Loss: 0.6317\n",
      "Epoch 146, Loss: 0.6305\n",
      "Epoch 147, Loss: 0.6306\n",
      "Epoch 148, Loss: 0.6370\n",
      "Epoch 149, Loss: 0.6359\n",
      "Epoch 150, Loss: 0.6301\n",
      "Epoch 151, Loss: 0.6305\n",
      "Epoch 152, Loss: 0.6359\n",
      "Epoch 153, Loss: 0.6415\n",
      "Epoch 154, Loss: 0.6368\n",
      "Epoch 155, Loss: 0.6312\n",
      "Epoch 156, Loss: 0.6443\n",
      "Epoch 157, Loss: 0.6441\n",
      "Epoch 158, Loss: 0.6483\n",
      "Epoch 159, Loss: 0.6374\n",
      "Epoch 160, Loss: 0.6468\n",
      "Epoch 161, Loss: 0.6334\n",
      "Epoch 162, Loss: 0.6321\n",
      "Epoch 163, Loss: 0.6325\n",
      "Epoch 164, Loss: 0.6277\n",
      "Epoch 165, Loss: 0.6351\n",
      "Epoch 166, Loss: 0.6358\n",
      "Epoch 167, Loss: 0.6361\n",
      "Epoch 168, Loss: 0.6369\n",
      "Epoch 169, Loss: 0.6328\n",
      "Epoch 170, Loss: 0.6442\n",
      "Epoch 171, Loss: 0.6360\n",
      "Epoch 172, Loss: 0.6301\n",
      "Epoch 173, Loss: 0.6264\n",
      "Epoch 174, Loss: 0.6310\n",
      "Epoch 175, Loss: 0.6351\n",
      "Epoch 176, Loss: 0.6416\n",
      "Epoch 177, Loss: 0.6415\n",
      "Epoch 178, Loss: 0.6322\n",
      "Epoch 179, Loss: 0.6343\n",
      "Epoch 180, Loss: 0.6433\n",
      "Epoch 181, Loss: 0.6321\n",
      "Epoch 182, Loss: 0.6430\n",
      "Epoch 183, Loss: 0.6382\n",
      "Epoch 184, Loss: 0.6410\n",
      "Epoch 185, Loss: 0.6352\n",
      "Epoch 186, Loss: 0.6299\n",
      "Epoch 187, Loss: 0.6302\n",
      "Epoch 188, Loss: 0.6379\n",
      "Epoch 189, Loss: 0.6228\n",
      "Epoch 190, Loss: 0.6227\n",
      "Epoch 191, Loss: 0.6292\n",
      "Epoch 192, Loss: 0.6397\n",
      "Epoch 193, Loss: 0.6407\n",
      "Epoch 194, Loss: 0.6277\n",
      "Epoch 195, Loss: 0.6365\n",
      "Epoch 196, Loss: 0.6383\n",
      "Epoch 197, Loss: 0.6389\n",
      "Epoch 198, Loss: 0.6390\n",
      "Epoch 199, Loss: 0.6457\n",
      "Epoch 200, Loss: 0.6426\n",
      "Epoch 201, Loss: 0.6294\n",
      "Epoch 202, Loss: 0.6400\n",
      "Epoch 203, Loss: 0.6387\n",
      "Epoch 204, Loss: 0.6300\n",
      "Epoch 205, Loss: 0.6380\n",
      "Epoch 206, Loss: 0.6355\n",
      "Epoch 207, Loss: 0.6332\n",
      "Epoch 208, Loss: 0.6336\n",
      "Epoch 209, Loss: 0.6441\n",
      "Epoch 210, Loss: 0.6396\n",
      "Epoch 211, Loss: 0.6376\n",
      "Epoch 212, Loss: 0.6497\n",
      "Epoch 213, Loss: 0.6337\n",
      "Epoch 214, Loss: 0.6331\n",
      "Epoch 215, Loss: 0.6281\n",
      "Epoch 216, Loss: 0.6297\n",
      "Epoch 217, Loss: 0.6412\n",
      "Epoch 218, Loss: 0.6491\n",
      "Epoch 219, Loss: 0.6623\n",
      "Epoch 220, Loss: 0.6428\n",
      "Epoch 221, Loss: 0.6273\n",
      "Epoch 222, Loss: 0.6321\n",
      "Epoch 223, Loss: 0.6336\n",
      "Epoch 224, Loss: 0.6323\n",
      "Epoch 225, Loss: 0.6502\n",
      "Epoch 226, Loss: 0.6277\n",
      "Epoch 227, Loss: 0.6320\n",
      "Epoch 228, Loss: 0.6182\n",
      "Epoch 229, Loss: 0.6257\n",
      "Epoch 230, Loss: 0.6212\n",
      "Epoch 231, Loss: 0.6479\n",
      "Epoch 232, Loss: 0.6389\n",
      "Epoch 233, Loss: 0.6481\n",
      "Epoch 234, Loss: 0.6456\n",
      "Epoch 235, Loss: 0.6257\n",
      "Epoch 236, Loss: 0.6384\n",
      "Epoch 237, Loss: 0.6352\n",
      "Epoch 238, Loss: 0.6383\n",
      "Epoch 239, Loss: 0.6365\n",
      "Epoch 240, Loss: 0.6302\n",
      "Epoch 241, Loss: 0.6475\n",
      "Epoch 242, Loss: 0.6567\n",
      "Epoch 243, Loss: 0.6549\n",
      "Epoch 244, Loss: 0.6391\n",
      "Epoch 245, Loss: 0.6450\n",
      "Epoch 246, Loss: 0.6426\n",
      "Epoch 247, Loss: 0.6463\n",
      "Epoch 248, Loss: 0.6389\n",
      "Epoch 249, Loss: 0.6461\n"
     ]
    }
   ],
   "source": [
    "x_dim = X.shape[1]\n",
    "model = CVAE(x_dim=x_dim, cond_dim=1, latent_dim=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 250\n",
    "last_values = [0] + [1] * 5\n",
    "threshold = 0.002\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(x_batch, y_batch.unsqueeze(1))\n",
    "        loss = loss_function(x_recon, x_batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        last_values[epoch%len(last_values)] = total_loss\n",
    "    \n",
    "    absolute_errors = [abs(x - max(last_values)) for x in last_values]\n",
    "    mae = sum(absolute_errors) / len(absolute_errors)\n",
    "    if mae < threshold:\n",
    "        print(f\"Early stopping at epoch {epoch} with MAE: {mae:.4f}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f91ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[239.81262    59.0088     60.193466  186.14357     4.2196136 987.9841\n",
      "  780.6705     26.420778 ]]\n"
     ]
    }
   ],
   "source": [
    "desired_strength = torch.tensor([[19.77]], dtype=torch.float32)\n",
    "\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.decode(z, desired_strength)\n",
    "    generated_original = scaler.inverse_transform(generated.numpy())\n",
    "\n",
    "print(generated_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b578b112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cement",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "slag",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ash",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "water",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "superplastic",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "coarseagg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fineagg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "strength",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "0fa5feb3-0e5d-4ccc-9d64-6b42260491b5",
       "rows": [
        [
         "0",
         "252.5",
         "0.0",
         "0.0",
         "185.7",
         "0.0",
         "1111.6",
         "784.3",
         "28",
         "19.77"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1111.6</td>\n",
       "      <td>784.3</td>\n",
       "      <td>28</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  slag  ash  water  superplastic  coarseagg  fineagg  age  strength\n",
       "0   252.5   0.0  0.0  185.7           0.0     1111.6    784.3   28     19.77"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['strength'] == 19.77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ec18f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,  86. , 116. , 118.3, 167. , 122. ,  71.5, 175. , 121.6,\n",
       "        24.5, 136.6, 187. , 112. ,  94. , 185.3,  94.6, 125.2, 172.4,\n",
       "        94.1, 132.1, 118.6, 106.9,  95.7, 150.4,  99.9, 174.7, 121.9,\n",
       "       123.8,  87.5,  77. , 125.1, 143. , 174.2, 127.9, 100.4,  96.2,\n",
       "        98. , 200.1,  95.6,  78.3,  96.7, 142. , 137.9, 113. , 100.5,\n",
       "       125.4, 121.4, 124.1, 183.9,  78. , 132. ,  97.4, 138.7, 138. ,\n",
       "       124.3, 185. , 141. , 195. , 132.6, 126. ,  93.9, 163.3, 173.5,\n",
       "       107. , 161. ,  60. , 163.8,  82. ,  98.8, 200. , 142.7, 118.2,\n",
       "        79. , 113.2, 106.2,  86.1, 128. , 158. ,  89.3, 123. , 130. ,\n",
       "       159.9, 142.8, 124.8, 111. ,  78.4,  92. , 111.9, 120. , 128.6,\n",
       "       109. , 100. , 143.6,  91. ,  89. ,  90. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.ash.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
