{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7fbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a94f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['strength'])\n",
    "y = df_train['strength']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a535bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, x_dim, cond_dim=1, latent_dim=3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(x_dim + cond_dim, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(32, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, x_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        xc = torch.cat([x, c], dim=1)\n",
    "        h = self.encoder(xc)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        zc = torch.cat([z, c], dim=1)\n",
    "        return self.decoder(zc)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z, c)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a51354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_recon, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='mean')\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3f5bac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 419.2252\n",
      "Epoch 1, Loss: 200.9539\n",
      "Epoch 2, Loss: 97.1265\n",
      "Epoch 3, Loss: 49.8083\n",
      "Epoch 4, Loss: 24.5046\n",
      "Epoch 5, Loss: 11.1290\n",
      "Epoch 6, Loss: 4.4139\n",
      "Epoch 7, Loss: 1.9119\n",
      "Epoch 8, Loss: 1.3778\n",
      "Epoch 9, Loss: 1.2774\n",
      "Epoch 10, Loss: 1.2475\n",
      "Epoch 11, Loss: 1.1953\n",
      "Epoch 12, Loss: 1.1935\n",
      "Epoch 13, Loss: 1.1540\n",
      "Epoch 14, Loss: 1.1894\n",
      "Epoch 15, Loss: 1.0558\n",
      "Epoch 16, Loss: 1.0527\n",
      "Epoch 17, Loss: 1.0619\n",
      "Epoch 18, Loss: 1.0633\n",
      "Epoch 19, Loss: 1.0661\n",
      "Epoch 20, Loss: 1.0270\n",
      "Epoch 21, Loss: 0.9880\n",
      "Epoch 22, Loss: 0.9400\n",
      "Epoch 23, Loss: 0.9816\n",
      "Epoch 24, Loss: 0.9368\n",
      "Epoch 25, Loss: 0.9390\n",
      "Epoch 26, Loss: 0.9032\n",
      "Epoch 27, Loss: 0.9126\n",
      "Epoch 28, Loss: 0.8797\n",
      "Epoch 29, Loss: 0.8859\n",
      "Epoch 30, Loss: 0.8811\n",
      "Epoch 31, Loss: 0.8362\n",
      "Epoch 32, Loss: 0.8202\n",
      "Epoch 33, Loss: 0.8227\n",
      "Epoch 34, Loss: 0.8307\n",
      "Epoch 35, Loss: 0.8074\n",
      "Epoch 36, Loss: 0.7946\n",
      "Epoch 37, Loss: 0.7875\n",
      "Epoch 38, Loss: 0.8020\n",
      "Epoch 39, Loss: 0.7733\n",
      "Epoch 40, Loss: 0.7854\n",
      "Epoch 41, Loss: 0.7605\n",
      "Epoch 42, Loss: 0.7776\n",
      "Epoch 43, Loss: 0.7477\n",
      "Epoch 44, Loss: 0.7352\n",
      "Epoch 45, Loss: 0.7438\n",
      "Epoch 46, Loss: 0.7334\n",
      "Epoch 47, Loss: 0.7241\n",
      "Epoch 48, Loss: 0.7268\n",
      "Epoch 49, Loss: 0.7256\n",
      "Epoch 50, Loss: 0.7024\n",
      "Epoch 51, Loss: 0.7129\n",
      "Epoch 52, Loss: 0.6942\n",
      "Epoch 53, Loss: 0.7117\n",
      "Epoch 54, Loss: 0.6971\n",
      "Epoch 55, Loss: 0.6865\n",
      "Epoch 56, Loss: 0.6886\n",
      "Epoch 57, Loss: 0.7101\n",
      "Epoch 58, Loss: 0.6847\n",
      "Epoch 59, Loss: 0.6804\n",
      "Epoch 60, Loss: 0.6877\n",
      "Epoch 61, Loss: 0.6630\n",
      "Epoch 62, Loss: 0.6717\n",
      "Epoch 63, Loss: 0.6704\n",
      "Epoch 64, Loss: 0.6712\n",
      "Epoch 65, Loss: 0.6667\n",
      "Epoch 66, Loss: 0.6755\n",
      "Epoch 67, Loss: 0.6758\n",
      "Epoch 68, Loss: 0.6603\n",
      "Epoch 69, Loss: 0.6683\n",
      "Epoch 70, Loss: 0.6596\n",
      "Epoch 71, Loss: 0.6507\n",
      "Epoch 72, Loss: 0.6593\n",
      "Epoch 73, Loss: 0.6572\n",
      "Epoch 74, Loss: 0.6499\n",
      "Epoch 75, Loss: 0.6463\n",
      "Epoch 76, Loss: 0.6431\n",
      "Epoch 77, Loss: 0.6612\n",
      "Epoch 78, Loss: 0.6519\n",
      "Epoch 79, Loss: 0.6437\n",
      "Epoch 80, Loss: 0.6509\n",
      "Epoch 81, Loss: 0.6552\n",
      "Epoch 82, Loss: 0.6553\n",
      "Epoch 83, Loss: 0.6426\n",
      "Epoch 84, Loss: 0.6535\n",
      "Epoch 85, Loss: 0.6509\n",
      "Epoch 86, Loss: 0.6590\n",
      "Epoch 87, Loss: 0.6580\n",
      "Epoch 88, Loss: 0.6634\n",
      "Epoch 89, Loss: 0.6670\n",
      "Epoch 90, Loss: 0.6505\n",
      "Epoch 91, Loss: 0.6495\n",
      "Epoch 92, Loss: 0.6411\n",
      "Epoch 93, Loss: 0.6600\n",
      "Epoch 94, Loss: 0.6508\n",
      "Epoch 95, Loss: 0.6604\n",
      "Epoch 96, Loss: 0.6496\n",
      "Epoch 97, Loss: 0.6385\n",
      "Epoch 98, Loss: 0.6409\n",
      "Epoch 99, Loss: 0.6414\n",
      "Epoch 100, Loss: 0.6449\n",
      "Epoch 101, Loss: 0.6466\n",
      "Epoch 102, Loss: 0.6398\n",
      "Epoch 103, Loss: 0.6339\n",
      "Epoch 104, Loss: 0.6423\n",
      "Epoch 105, Loss: 0.6421\n",
      "Epoch 106, Loss: 0.6365\n",
      "Epoch 107, Loss: 0.6230\n",
      "Epoch 108, Loss: 0.6329\n",
      "Epoch 109, Loss: 0.6461\n",
      "Epoch 110, Loss: 0.6289\n",
      "Epoch 111, Loss: 0.6424\n",
      "Epoch 112, Loss: 0.6507\n",
      "Epoch 113, Loss: 0.6322\n",
      "Epoch 114, Loss: 0.6420\n",
      "Epoch 115, Loss: 0.6433\n",
      "Epoch 116, Loss: 0.6400\n",
      "Epoch 117, Loss: 0.6509\n",
      "Epoch 118, Loss: 0.6459\n",
      "Epoch 119, Loss: 0.6390\n",
      "Epoch 120, Loss: 0.6469\n",
      "Epoch 121, Loss: 0.6378\n",
      "Epoch 122, Loss: 0.6355\n",
      "Epoch 123, Loss: 0.6390\n",
      "Epoch 124, Loss: 0.6309\n",
      "Epoch 125, Loss: 0.6378\n",
      "Epoch 126, Loss: 0.6441\n",
      "Epoch 127, Loss: 0.6340\n",
      "Epoch 128, Loss: 0.6352\n",
      "Epoch 129, Loss: 0.6351\n",
      "Epoch 130, Loss: 0.6368\n",
      "Epoch 131, Loss: 0.6392\n",
      "Epoch 132, Loss: 0.6259\n",
      "Epoch 133, Loss: 0.6277\n",
      "Epoch 134, Loss: 0.6294\n",
      "Epoch 135, Loss: 0.6439\n",
      "Epoch 136, Loss: 0.6490\n",
      "Epoch 137, Loss: 0.6503\n",
      "Epoch 138, Loss: 0.6516\n",
      "Epoch 139, Loss: 0.6571\n",
      "Epoch 140, Loss: 0.6451\n",
      "Epoch 141, Loss: 0.6549\n",
      "Epoch 142, Loss: 0.6483\n",
      "Epoch 143, Loss: 0.6406\n",
      "Epoch 144, Loss: 0.6514\n",
      "Epoch 145, Loss: 0.6359\n",
      "Epoch 146, Loss: 0.6356\n",
      "Epoch 147, Loss: 0.6489\n",
      "Epoch 148, Loss: 0.6386\n",
      "Epoch 149, Loss: 0.6287\n",
      "Epoch 150, Loss: 0.6379\n",
      "Epoch 151, Loss: 0.6344\n",
      "Epoch 152, Loss: 0.6342\n",
      "Epoch 153, Loss: 0.6289\n",
      "Epoch 154, Loss: 0.6332\n",
      "Epoch 155, Loss: 0.6464\n",
      "Epoch 156, Loss: 0.6280\n",
      "Epoch 157, Loss: 0.6239\n",
      "Epoch 158, Loss: 0.6380\n",
      "Epoch 159, Loss: 0.6459\n",
      "Epoch 160, Loss: 0.6468\n",
      "Epoch 161, Loss: 0.6257\n",
      "Epoch 162, Loss: 0.6370\n",
      "Epoch 163, Loss: 0.6380\n",
      "Epoch 164, Loss: 0.6320\n",
      "Epoch 165, Loss: 0.6356\n",
      "Epoch 166, Loss: 0.6306\n",
      "Epoch 167, Loss: 0.6363\n",
      "Epoch 168, Loss: 0.6568\n",
      "Epoch 169, Loss: 0.6417\n",
      "Epoch 170, Loss: 0.6447\n",
      "Epoch 171, Loss: 0.6367\n",
      "Epoch 172, Loss: 0.6410\n",
      "Epoch 173, Loss: 0.6538\n",
      "Epoch 174, Loss: 0.6458\n",
      "Epoch 175, Loss: 0.6456\n",
      "Epoch 176, Loss: 0.6273\n",
      "Epoch 177, Loss: 0.6309\n",
      "Epoch 178, Loss: 0.6375\n",
      "Epoch 179, Loss: 0.6295\n",
      "Epoch 180, Loss: 0.6370\n",
      "Epoch 181, Loss: 0.6360\n",
      "Epoch 182, Loss: 0.6361\n",
      "Epoch 183, Loss: 0.6554\n",
      "Epoch 184, Loss: 0.6486\n",
      "Epoch 185, Loss: 0.6368\n",
      "Epoch 186, Loss: 0.6589\n",
      "Epoch 187, Loss: 0.6454\n",
      "Epoch 188, Loss: 0.6295\n",
      "Epoch 189, Loss: 0.6383\n",
      "Epoch 190, Loss: 0.6377\n",
      "Epoch 191, Loss: 0.6430\n",
      "Epoch 192, Loss: 0.6303\n",
      "Epoch 193, Loss: 0.6340\n",
      "Epoch 194, Loss: 0.6467\n",
      "Epoch 195, Loss: 0.6281\n",
      "Epoch 196, Loss: 0.6330\n",
      "Epoch 197, Loss: 0.6286\n",
      "Epoch 198, Loss: 0.6439\n",
      "Epoch 199, Loss: 0.6383\n",
      "Epoch 200, Loss: 0.6320\n",
      "Epoch 201, Loss: 0.6236\n",
      "Epoch 202, Loss: 0.6292\n",
      "Epoch 203, Loss: 0.6540\n",
      "Epoch 204, Loss: 0.6484\n",
      "Epoch 205, Loss: 0.6473\n",
      "Epoch 206, Loss: 0.6431\n",
      "Epoch 207, Loss: 0.6450\n",
      "Epoch 208, Loss: 0.6367\n",
      "Epoch 209, Loss: 0.6247\n",
      "Epoch 210, Loss: 0.6451\n",
      "Epoch 211, Loss: 0.6437\n",
      "Epoch 212, Loss: 0.6449\n",
      "Epoch 213, Loss: 0.6264\n",
      "Epoch 214, Loss: 0.6408\n",
      "Epoch 215, Loss: 0.6355\n",
      "Epoch 216, Loss: 0.6407\n",
      "Epoch 217, Loss: 0.6352\n",
      "Epoch 218, Loss: 0.6373\n",
      "Epoch 219, Loss: 0.6421\n",
      "Epoch 220, Loss: 0.6298\n",
      "Epoch 221, Loss: 0.6388\n",
      "Epoch 222, Loss: 0.6292\n",
      "Epoch 223, Loss: 0.6343\n",
      "Epoch 224, Loss: 0.6311\n",
      "Epoch 225, Loss: 0.6274\n",
      "Epoch 226, Loss: 0.6328\n",
      "Epoch 227, Loss: 0.6376\n",
      "Epoch 228, Loss: 0.6510\n",
      "Epoch 229, Loss: 0.6316\n",
      "Epoch 230, Loss: 0.6394\n",
      "Epoch 231, Loss: 0.6514\n",
      "Epoch 232, Loss: 0.6355\n",
      "Epoch 233, Loss: 0.6407\n",
      "Epoch 234, Loss: 0.6458\n",
      "Epoch 235, Loss: 0.6445\n",
      "Epoch 236, Loss: 0.6345\n",
      "Epoch 237, Loss: 0.6431\n",
      "Epoch 238, Loss: 0.6459\n",
      "Epoch 239, Loss: 0.6327\n",
      "Epoch 240, Loss: 0.6449\n",
      "Epoch 241, Loss: 0.6515\n",
      "Epoch 242, Loss: 0.6353\n",
      "Epoch 243, Loss: 0.6383\n",
      "Epoch 244, Loss: 0.6399\n",
      "Epoch 245, Loss: 0.6528\n",
      "Epoch 246, Loss: 0.6482\n",
      "Epoch 247, Loss: 0.6287\n",
      "Epoch 248, Loss: 0.6261\n",
      "Epoch 249, Loss: 0.6320\n"
     ]
    }
   ],
   "source": [
    "x_dim = X.shape[1]\n",
    "model = CVAE(x_dim=x_dim, cond_dim=1, latent_dim=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 250\n",
    "last_values = [0] + [1] * 5\n",
    "threshold = 0.002\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(x_batch, y_batch.unsqueeze(1))\n",
    "        loss = loss_function(x_recon, x_batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        last_values[epoch%len(last_values)] = total_loss\n",
    "    \n",
    "    absolute_errors = [abs(x - max(last_values)) for x in last_values]\n",
    "    mae = sum(absolute_errors) / len(absolute_errors)\n",
    "    if mae < threshold:\n",
    "        print(f\"Early stopping at epoch {epoch} with MAE: {mae:.4f}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f91ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[224.92482   70.61229   71.09925  185.87529    4.91794  985.2465\n",
      "  803.50586   41.930157]]\n"
     ]
    }
   ],
   "source": [
    "desired_strength = torch.tensor([[19.77]], dtype=torch.float32)\n",
    "\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generated = model.decode(z, desired_strength)\n",
    "    generated_original = scaler.inverse_transform(generated.numpy())\n",
    "\n",
    "print(generated_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b578b112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1111.6</td>\n",
       "      <td>784.3</td>\n",
       "      <td>28</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement  slag  ash  water  superplastic  coarseagg  fineagg  age  strength\n",
       "0   252.5   0.0  0.0  185.7           0.0     1111.6    784.3   28     19.77"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['strength'] == 19.77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ec18f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,  86. , 116. , 118.3, 167. , 122. ,  71.5, 175. , 121.6,\n",
       "        24.5, 136.6, 187. , 112. ,  94. , 185.3,  94.6, 125.2, 172.4,\n",
       "        94.1, 132.1, 118.6, 106.9,  95.7, 150.4,  99.9, 174.7, 121.9,\n",
       "       123.8,  87.5,  77. , 125.1, 143. , 174.2, 127.9, 100.4,  96.2,\n",
       "        98. , 200.1,  95.6,  78.3,  96.7, 142. , 137.9, 113. , 100.5,\n",
       "       125.4, 121.4, 124.1, 183.9,  78. , 132. ,  97.4, 138.7, 138. ,\n",
       "       124.3, 185. , 141. , 195. , 132.6, 126. ,  93.9, 163.3, 173.5,\n",
       "       107. , 161. ,  60. , 163.8,  82. ,  98.8, 200. , 142.7, 118.2,\n",
       "        79. , 113.2, 106.2,  86.1, 128. , 158. ,  89.3, 123. , 130. ,\n",
       "       159.9, 142.8, 124.8, 111. ,  78.4,  92. , 111.9, 120. , 128.6,\n",
       "       109. , 100. , 143.6,  91. ,  89. ,  90. ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.ash.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
